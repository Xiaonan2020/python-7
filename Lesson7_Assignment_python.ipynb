{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img.kaikeba.com/web/kkb_index/img_index_logo.png)\n",
    "# 基础课第一部分（python）第七次作业\n",
    "\n",
    "各位同学大家好！今天课上演示爬虫的原理，我们来回顾一下爬虫的思路，进行爬虫练习。\n",
    "爬虫是一个程序，这个程序可以获得网页数据。\n",
    "## 爬虫的思路\n",
    "- 1.首先确定需要爬取的网URL地址   \n",
    "[空气质量指数(http://www.tianqihoubao.com/aqi/)](http://www.tianqihoubao.com/aqi/)    \n",
    "\n",
    "\n",
    "- 2.通过HTTP/HTTPS协议来获取对应的HTML页面  \n",
    "\n",
    "\n",
    "- 3.提取HTML页面内有用的数据：\n",
    "- a. 如果是需要的数据--保存\n",
    "- b. 如果有其他URL，继续执行第二步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬虫练习 \n",
    "爬虫项目整体代码：  \n",
    "[高民权_中国城市空气质量数据抓取_Github](https://github.com/fortyMiles/ChineseAirConditionCrawler)  \n",
    "【没有头绪的指令】Github中的`get_location_info.py`文件对应city_coding的生成  \n",
    "\n",
    "**处理城市编码**  \n",
    "将`<div class=\"citychk\">`copy下来，进一步处理，生成 city_coding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373 369\n",
      "write done!\n"
     ]
    }
   ],
   "source": [
    "# 请大家尽量尝试自己从网页中搜寻并copy，而不是直接从课件copy\n",
    "\n",
    "# 借鉴高老师Github中的get_location_info.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_location_html():\n",
    "    http_url = 'http://www.tianqihoubao.com/aqi/'\n",
    "    r = requests.get(http_url)\n",
    "    r.encoding = 'GBK'\n",
    "    html_file = r.text\n",
    "    return html_file\n",
    "\n",
    "# print(get_location_html())\n",
    "\n",
    "def parse_location(html_doc):\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    CITY_CHECK = '.citychk > dl > dd > a'\n",
    "\n",
    "    city_map = []\n",
    "\n",
    "    city_check = soup.select(CITY_CHECK)\n",
    "#     print(city_check)\n",
    "\n",
    "    for c in city_check:\n",
    "#         print(c,'\\n')\n",
    "#         print(c['href'],'\\n')\n",
    "        coding = c['href'].split('/')[2].replace('.html', '')\n",
    "        city = c.string\n",
    "        if len(city) > 7 or len(city) == 0 : continue\n",
    "        city_map.append((city, coding))\n",
    "\n",
    "    return city_map\n",
    "\n",
    "# parse_location(get_location_html())\n",
    "\n",
    "def save_city_map(city_map):\n",
    "    with open('./city_coding.txt', 'w+') as f:\n",
    "        for c in city_map:\n",
    "            if c:\n",
    "                f.write(c[0] + '\\t' + c[1] + '\\n')\n",
    "\n",
    "    print('write done!')\n",
    "\n",
    "city_coding = parse_location(get_location_html())    \n",
    "print(len(city_coding), len(set(city_coding)))\n",
    "    \n",
    "save_city_map(city_coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先安装包**  \n",
    "\n",
    "``` bash\n",
    "pip install bs4\n",
    "```\n",
    "参考：[Beautiful Soup 4.2.0 文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)\n",
    "\n",
    "** 读取city_coding **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'北京': 'beijing', '天津': 'tianjin', '上海': 'shanghai', '重庆': 'chongqing', '广州': 'guangzhou', '深圳': 'shenzhen', '杭州': 'hangzhou', '成都': 'chengdu', '石家庄': 'shijiazhuang', '唐山': 'tangshan', '秦皇岛': 'qinhuangdao', '保定': 'baoding', '张家口': 'zhangjiakou', '邯郸': 'handan', '邢台': 'xingtai', '承德': 'chengde', '沧州': 'cangzhou', '廊坊': 'langfang', '衡水': 'hengshui', '太原': 'taiyuan', '大同': 'datong', '阳泉': 'yangquan', '长治': 'changzhi', '临汾': 'linfen', '晋城': 'jincheng', '朔州': 'shuozhou', '运城': 'sxyuncheng', '忻州': 'xinzhou', '吕梁': 'lvliang', '晋中': 'jinzhong', '呼和浩特': 'huhehaote', '包头': 'baotou', '鄂尔多斯': 'eerduosi', '乌海': 'wuhai', '赤峰': 'chifeng', '通辽': 'tongliao', '巴彦淖尔': 'bayannaoer', '兴安盟': 'xinganmeng', '阿拉善盟': 'alashanmeng', '呼伦贝尔': 'hulunbeier', '二连浩特': 'erlianhaote', '锡林郭勒': 'xilinguole', '沈阳': 'shenyang', '大连': 'dalian', '丹东': 'dandong', '营口': 'yingkou', '盘锦': 'panjin', '葫芦岛': 'huludao', '鞍山': 'anshan', '锦州': 'jinzhou', '本溪': 'benxi', '瓦房店': 'wafangdian', '抚顺': 'fushun', '辽阳': 'liaoyang', '阜新': 'fuxin', '朝阳': 'chaoyang', '铁岭': 'tieling', '长春': 'changchun', '吉林': 'jilin', '四平': 'siping', '辽源': 'liaoyuan', '白山': 'baishan', '松原': 'songyuan', '白城': 'baicheng', '延边': 'yanbian', '通化': 'tonghua', '哈尔滨': 'haerbin', '齐齐哈尔': 'qiqihaer', '鸡西': 'jixi', '鹤岗': 'hegang', '双鸭山': 'shuangyashan', '大庆': 'daqing', '佳木斯': 'jiamusi', '七台河': 'qitaihe', '牡丹江': 'mudanjiang', '黑河': 'heihe', '绥化': 'suihua', '大兴安岭': 'daxinganling', '伊春': 'yichun', '甘南': 'gannan', '南京': 'nanjing', '无锡': 'wuxi', '徐州': 'xuzhou', '常州': 'changzhou', '苏州': 'suzhou', '南通': 'nantong', '连云港': 'lianyungang', '淮安': 'huaian', '盐城': 'yancheng', '扬州': 'yangzhou', '镇江': 'zhenjiang', '泰州': 'jstaizhou', '宿迁': 'suqian', '昆山': 'kunshan', '海门': 'haimen', '太仓': 'taicang', '江阴': 'jiangyin', '溧阳': 'liyang', '金坛': 'jintan', '宜兴': 'yixing', '句容': 'jurong', '常熟': 'changshu', '吴江': 'wujiang', '张家港': 'zhangjiagang', '宁波': 'ningbo', '温州': 'wenzhou', '嘉兴': 'jiaxing', '湖州': 'huzhou', '金华': 'jinhua', '衢州': 'quzhou', '舟山': 'zhoushan', '台州': 'taizhou', '丽水': 'lishui', '绍兴': 'shaoxing', '义乌': 'yiwu', '富阳': 'zjfuyang', '临安': 'linan', '合肥': 'hefei', '芜湖': 'wuhu', '蚌埠': 'bangbu', '淮南': 'huainan', '马鞍山': 'maanshan', '淮北': 'huaibei', '铜陵': 'tongling', '安庆': 'anqing', '黄山': 'huangshan', '滁州': 'chuzhou', '阜阳': 'fuyang', '宿州': 'anhuisuzhou', '巢湖': 'chaohu', '六安': 'liuan', '亳州': 'bozhou', '池州': 'chizhou', '宣城': 'xuancheng', '福州': 'fujianfuzhou', '厦门': 'xiamen', '泉州': 'quanzhou', '莆田': 'putian', '三明': 'sanming', '漳州': 'zhangzhou', '南平': 'nanping', '龙岩': 'longyan', '宁德': 'ningde', '南昌': 'nanchang', '景德镇': 'jingdezhen', '萍乡': 'pingxiang', '新余': 'xinyu', '鹰潭': 'yingtan', '赣州': 'ganzhou', '宜春': 'jxyichun', '抚州': 'fuzhou', '九江': 'jiujiang', '上饶': 'shangrao', '吉安': 'jian', '济南': 'jinan', '青岛': 'qingdao', '淄博': 'zibo', '枣庄': 'zaozhuang', '东营': 'dongying', '烟台': 'yantai', '潍坊': 'weifang', '济宁': 'sdjining', '泰安': 'taian', '威海': 'weihai', '日照': 'rizhao', '莱芜': 'laiwu', '临沂': 'linyi', '德州': 'dezhou', '聊城': 'liaocheng', '滨州': 'binzhou', '菏泽': 'heze', '乳山': 'rushan', '荣成': 'sdrongcheng', '文登': 'wendeng', '章丘': 'zhangqiu', '平度': 'pingdu', '莱州': 'laizhou', '招远': 'sdzhaoyuan', '莱西': 'laixi', '胶州': 'jiaozhou', '蓬莱': 'penglai', '胶南': 'jiaonan', '寿光': 'shouguang', '即墨': 'jimo', '郑州': 'zhengzhou', '洛阳': 'lvyang', '平顶山': 'pingdingshan', '鹤壁': 'hebi', '焦作': 'jiaozuo', '漯河': 'luohe', '三门峡': 'sanmenxia', '南阳': 'nanyang', '商丘': 'shangqiu', '信阳': 'xinyang', '周口': 'zhoukou', '驻马店': 'zhumadian', '安阳': 'anyang', '开封': 'kaifeng', '濮阳': 'puyang', '许昌': 'xuchang', '新乡': 'xinxiang', '武汉': 'wuhan', '十堰': 'shiyan', '宜昌': 'yichang', '鄂州': 'ezhou', '荆门': 'jingmen', '孝感': 'xiaogan', '黄冈': 'huanggang', '咸宁': 'xianning', '黄石': 'huangshi', '恩施': 'enshi', '襄阳': 'xiangyang', '随州': 'suizhou', '荆州': 'jingzhou', '长沙': 'changsha', '株洲': 'zhuzhou', '湘潭': 'xiangtan', '常德': 'changde', '张家界': 'zhangjiajie', '益阳': 'yiyang', '郴州': 'chenzhou', '永州': 'yongzhou', '怀化': 'huaihua', '娄底': 'loudi', '邵阳': 'shaoyang', '岳阳': 'yueyang', '湘西': 'xiangxi', '衡阳': 'hengyang', '韶关': 'shaoguan', '珠海': 'zhuhai', '汕头': 'shantou', '佛山': 'foshan', '江门': 'jiangmen', '肇庆': 'zhaoqing', '惠州': 'huizhou', '河源': 'heyuan', '清远': 'gdqingyuan', '东莞': 'dongguang', '中山': 'zhongshan', '湛江': 'zhanjiang', '茂名': 'maoming', '梅州': 'meizhou', '汕尾': 'shanwei', '阳江': 'yangjiang', '潮州': 'chaozhou', '揭阳': 'jieyang', '云浮': 'yunfu', '南宁': 'nanning', '柳州': 'liuzhou', '北海': 'beihai', '桂林': 'guilin', '梧州': 'wuzhou', '防城港': 'fangchenggang', '钦州': 'gxqinzhou', '贵港': 'guigang', '玉林': 'guangxiyulin', '百色': 'baise', '贺州': 'hezhou', '河池': 'hechi', '来宾': 'laibin', '崇左': 'chongzuo', '海口': 'haikou', '三亚': 'sanya', '自贡': 'zigong', '攀枝花': 'panzhihua', '泸州': 'luzhou', '德阳': 'deyang', '绵阳': 'mianyang', '广元': 'guangyuan', '遂宁': 'scsuining', '乐山': 'leshan', '南充': 'nanchong', '眉山': 'meishan', '达州': 'dazhou', '雅安': 'yaan', '巴中': 'bazhong', '资阳': 'ziyang', '甘孜': 'ganzi', '内江': 'neijiang', '宜宾': 'yibin', '广安': 'guangan', '阿坝': 'aba', '凉山': 'liangshan', '贵阳': 'guiyang', '六盘水': 'liupanshui', '遵义': 'zunyi', '安顺': 'anshun', '毕节': 'bijie', '铜仁': 'tongren', '黔西南': 'qianxinan', '黔南': 'qiannan', '黔东南': 'qiandongnan', '昆明': 'kunming', '玉溪': 'yuxi', '保山': 'baoshan', '昭通': 'zhaotong', '丽江': 'lijiang', '临沧': 'lincang', '西双版纳': 'xishuangbanna', '德宏': 'dehong', '怒江': 'nujiang', '大理': 'dali', '曲靖': 'qujing', '楚雄': 'chuxiong', '红河': 'honghe', '思茅': 'simao', '文山': 'wenshan', '普洱': 'puer', '迪庆': 'diqing', '拉萨': 'lasa', '林芝': 'linzhi', '山南': 'shannan', '昌都': 'changdu', '日喀则': 'rikaze', '阿里': 'ali', '那曲': 'naqu', '西安': 'xian', '铜川': 'tongchuan', '宝鸡': 'baoji', '咸阳': 'xianyang', '渭南': 'weinan', '延安': 'yanan', '汉中': 'hanzhong', '榆林': 'yulin', '安康': 'ankang', '商洛': 'shanglv', '兰州': 'lanzhou', '嘉峪关': 'jiayuguan', '天水': 'tianshui', '武威': 'wuwei', '张掖': 'zhangye', '平凉': 'pingliang', '酒泉': 'jiuquan', '庆阳': 'gsqingyang', '定西': 'dingxi', '临夏': 'linxia', '白银': 'baiyin', '金昌': 'jinchang', '陇南': 'longnan', '西宁': 'xining', '海东': 'haidong', '果洛': 'guolv', '海北': 'haibei', '海南': 'hainan', '海西': 'haixi', '玉树': 'yushu', '黄南': 'huangnan', '银川': 'yinchuan', '石嘴山': 'shizuishan', '吴忠': 'wuzhong', '固原': 'nxguyuan', '中卫': 'zhongwei', '乌鲁木齐': 'wulumuqi', '伊犁哈萨克州': 'yili', '克拉玛依': 'kelamayi', '哈密': 'hami', '石河子': 'shihezi', '和田': 'hetian', '五家渠': 'wujiaqu', '阿克苏': 'akesu', '阿勒泰': 'aletai', '喀什': 'kashi', '库尔勒': 'kuerle', '吐鲁番': 'tulufan', '塔城': 'tacheng', '博州': 'xjbozhou', '昌吉': 'changji', '克州': 'kezhou'}\n"
     ]
    }
   ],
   "source": [
    "def get_city_coding(file='./city_coding.txt'):\n",
    "    #your code\n",
    "    \n",
    "    city_coding = {}\n",
    "    with open(file) as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            try: \n",
    "                city, coding = line.split('\\t')\n",
    "                city_coding[city.strip()] = coding.strip()\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    return city_coding\n",
    "\n",
    "\n",
    "city_coding = get_city_coding()\n",
    "# print(city_coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 指定城市的URL地址确定 **\n",
    "- 如何拼接成自己想要的URL地址？  \n",
    "  如果是当前月份可以看到直接使用城市名称即可，如 http://www.tianqihoubao.com/aqi/hangzhou.html  \n",
    "  如果查询的是历史月份，可以看到是这种格式 http://www.tianqihoubao.com/aqi/hangzhou-201702.html   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.tianqihoubao.com/aqi/hangzhou.html\n",
      "http://www.tianqihoubao.com/aqi/hangzhou-201805.html\n"
     ]
    }
   ],
   "source": [
    "def build_url(city_coding, year=None, month=None):#输入城市 年 月 输出 url地址\n",
    "    BASE = 'http://www.tianqihoubao.com/aqi/'\n",
    "    city_base_url = BASE + '{}.html'\n",
    "    city_data_base_url = BASE + '{}-{}{}.html'\n",
    "    \n",
    "    if year is not None and month is not None:\n",
    "        month = str(month) if month >= 10 else '0' + str(month)\n",
    "        return city_data_base_url.format(city_coding, year, month)\n",
    "    else:\n",
    "        return city_base_url.format(city_coding)\n",
    "    \n",
    "hangzhou = city_coding['杭州']\n",
    "print(build_url(hangzhou))\n",
    "print(build_url(hangzhou, 2018, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** HTTP 请求状态了解 **\n",
    "- 200 - 请求成功\n",
    "- 404 - 请求的资源（网页等）不存在\n",
    "- 403 - 服务器理解请求客户端的请求，但是拒绝执行此请求     \n",
    "\n",
    "** 模拟浏览器发送http请求 **\n",
    "- get post\n",
    "\n",
    "** 获得响应的数据 分析 保存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('city', '日期', '质量等级', 'AQI指数', '当天AQI排名', 'PM2.5', 'PM10', 'So2', 'No2', 'Co', 'O3'), ('杭州', '2019-09-01', '优', '27', '66', '12', '18', '4', '24', '0.64', '80'), ('杭州', '2019-09-02', '优', '22', '52', '9', '15', '4', '35', '0.72', '32'), ('杭州', '2019-09-03', '优', '30', '104', '17', '29', '4', '41', '0.73', '15'), ('杭州', '2019-09-04', '优', '49', '238', '32', '50', '5', '48', '0.98', '27'), ('杭州', '2019-09-05', '优', '34', '124', '19', '33', '4', '33', '0.77', '34'), ('杭州', '2019-09-06', '优', '23', '40', '11', '22', '4', '23', '0.60', '36'), ('杭州', '2019-09-07', '优', '35', '84', '18', '32', '4', '24', '0.62', '74'), ('杭州', '2019-09-08', '良', '80', '288', '46', '78', '5', '43', '0.85', '112'), ('杭州', '2019-09-09', '良', '78', '309', '47', '77', '5', '44', '0.88', '100'), ('杭州', '2019-09-10', '良', '79', '344', '50', '84', '6', '49', '0.83', '103'), ('杭州', '2019-09-11', '良', '71', '344', '41', '70', '5', '39', '0.75', '100'), ('杭州', '2019-09-12', '良', '57', '299', '32', '52', '5', '26', '0.60', '98'), ('杭州', '2019-09-13', '良', '54', '284', '24', '40', '4', '20', '0.58', '119'), ('杭州', '2019-09-14', '优', '37', '180', '18', '34', '4', '20', '0.53', '93'), ('杭州', '2019-09-15', '优', '44', '233', '24', '45', '5', '28', '0.75', '72'), ('杭州', '2019-09-16', '优', '47', '250', '17', '35', '6', '26', '0.61', '110'), ('杭州', '2019-09-17', '良', '56', '274', '28', '53', '6', '32', '0.75', '106'), ('杭州', '2019-09-18', '优', '41', '172', '13', '38', '6', '25', '0.55', '104'), ('杭州', '2019-09-19', '优', '45', '202', '16', '45', '6', '24', '0.56', '99'), ('杭州', '2019-09-20', '优', '45', '176', '17', '41', '6', '29', '0.56', '96'), ('杭州', '2019-09-21', '优', '38', '118', '16', '38', '5', '26', '0.63', '81'), ('杭州', '2019-09-22', '优', '35', '69', '15', '35', '4', '22', '0.59', '91'), ('杭州', '2019-09-23', '良', '53', '184', '24', '54', '6', '38', '0.73', '98'), ('杭州', '2019-09-24', '良', '68', '271', '33', '66', '7', '56', '0.79', '99'), ('杭州', '2019-09-25', '良', '78', '299', '44', '80', '7', '59', '0.95', '108'), ('杭州', '2019-09-26', '良', '91', '355', '60', '100', '7', '49', '0.93', '122'), ('杭州', '2019-09-27', '良', '61', '207', '33', '57', '6', '46', '0.65', '88'), ('杭州', '2019-09-28', '良', '70', '212', '39', '69', '6', '57', '0.68', '90'), ('杭州', '2019-09-29', '良', '80', '260', '52', '85', '6', '56', '0.74', '90'), ('杭州', '2019-09-30', '良', '56', '131', '33', '53', '6', '32', '0.60', '88')]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def parse(url, city_name, city_ENname):\n",
    "    # your code\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html)\n",
    "        data_table = soup.table\n",
    "\n",
    "        name_index = 1\n",
    "        content = data_table.contents[name_index:]\n",
    "#         print(content)\n",
    "\n",
    "        result = []\n",
    "        for index, c in enumerate(content[::2]):\n",
    "            if index == 0:\n",
    "                result.append(tuple(['city'] + c.text.split()))\n",
    "            else:\n",
    "                result.append(tuple([city_ENname] + c.text.split()))\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "        print('Network Error:', response.status_code)\n",
    "\n",
    "def city_ENname(want_city):\n",
    "    for i in city_coding:\n",
    "        if city_coding[i] == want_city :\n",
    "            return i\n",
    "        \n",
    "city_coding = get_city_coding()\n",
    "want_city = city_coding['杭州']\n",
    "city_ENname = city_ENname(want_city)\n",
    "url = build_url(want_city, 2019, 9)\n",
    "result = parse(url, want_city, city_ENname)\n",
    "print(result)\n",
    "\n",
    "def save_csv(file, data):\n",
    "    # your code \n",
    "    if data == None or len(data) == 1: return\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(data[1:])\n",
    "    else:\n",
    "        with open(file, 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(data)\n",
    "\n",
    "save_csv(f'./{want_city}.csv', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 整体流程 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_all():\n",
    "    file = './data/allcity_2019.csv'\n",
    "    \n",
    "    city_codings = get_city_coding()\n",
    "    allcities = list(city_codings.keys())\n",
    "    \n",
    "    for city in allcities:\n",
    "        city_code = city_codings[city]\n",
    "        for year in range(2019,2012,-1):\n",
    "            for month in range(1,13):\n",
    "                url = build_url(city_code, year, month)\n",
    "                result = parse(url, city_code) # city\n",
    "                print(f'\\r{city} {year}-{month} {len(result)}', end='')\n",
    "                save_csv(file, result)\n",
    "                time.sleep(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawler_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
